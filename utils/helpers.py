import re


def fix_all_caps_text(text: str) -> str:
    if not text:
        return ""
    
    lines = text.split('\n')
    fixed_lines = []
    
    for line in lines:
        line = line.strip()
        if not line:
            fixed_lines.append('')
            continue
        
        if line.isupper():
            abbreviations = re.findall(r'\b[A-ZÄžÃœÅžÃ–Ã‡I]{2,5}\b', line)
            
            fixed = line.title()
            
            for abbr in abbreviations:
                known_abbrs = ['NATO', 'AB', 'TRT', 'BBC', 'CNN', 'AKP', 'CHP', 'MHP', 
                              'Ä°YÄ°', 'HDP', 'PKK', 'YPG', 'IÅžÄ°D', 'DEAÅž', 'ABD', 'AB',
                              'TBMM', 'AÄ°HM', 'UEFA', 'FIFA', 'NBA', 'NFL']
                
                if abbr in known_abbrs:
                    fixed = fixed.replace(abbr.title(), abbr)
            
            fixed_lines.append(fixed)
        else:
            fixed_lines.append(line)
    
    return '\n'.join(fixed_lines)


def remove_clickbait_phrases(text: str) -> str:
    if not text:
        return ""
    
    clickbait_patterns = [
        r'TIKLAYIN[!\s]*',
        r'ÅžOKA UÄžRAYACAKSINIZ[!\s]*',
        r'Ä°NANILMAZ[!\s]*',
        r'MUTLAKA Ä°ZLEYÄ°N[!\s]*',
        r'SOSYAL MEDYAYI SALLADI[!\s]*',
        r'PAYLAÅžIM REKORU KIRDI[!\s]*',
        r'VÄ°RAL OLDU[!\s]*',
        r'ORTALIK KARIÅžTI[!\s]*',
        r'BU HABER BOMBA GÄ°BÄ°[!\s]*',
        r'SIR DEÅžÄ°FRE OLDU[!\s]*',
        r'FLAÅž[!\s]*',
        r'SON DAKÄ°KA[!\s]*(?!:)',
        r'ACÄ°L[!\s]*',
    ]
    
    for pattern in clickbait_patterns:
        text = re.sub(pattern, '', text, flags=re.IGNORECASE)
    
    return text


def normalize_whitespace(text: str) -> str:
    if not text:
        return ""
    
    text = text.replace('\t', ' ')
    
    text = re.sub(r' +', ' ', text)
    
    lines = text.split('\n')
    lines = [line.strip() for line in lines]
    
    text = '\n'.join(lines)
    text = re.sub(r'\n{3,}', '\n\n', text)
    
    return text.strip()


def fix_punctuation_spacing(text: str) -> str:
    if not text:
        return ""
    
    text = re.sub(r'\s+([.,!?;:])', r'\1', text)
    
    text = re.sub(r'([.,!?;:])([A-Za-zÄŸÃ¼ÅŸÄ±Ã¶Ã§ÄžÃœÅžÄ°Ã–Ã‡])', r'\1 \2', text)
    
    text = re.sub(r'\s+([)\]])', r'\1', text)
    text = re.sub(r'([([])\s+', r'\1', text)
    
    return text


def remove_metadata_lines(text: str) -> str:
    if not text:
        return ""
    
    metadata_patterns = [
        r'^Yazar\s*[:=]\s*.*$',
        r'^EditÃ¶r\s*[:=]\s*.*$',
        r'^Editor\s*[:=]\s*.*$',
        r'^Kaynak\s*[:=]\s*.*$',
        r'^Source\s*[:=]\s*.*$',
        r'^Foto\s*[:=]\s*.*$',
        r'^Photo\s*[:=]\s*.*$',
        r'^FotoÄŸraf\s*[:=]\s*.*$',
        r'^GÃ¶rsel\s*[:=]\s*.*$',
        r'^Tarih\s*[:=]\s*.*$',
        r'^Date\s*[:=]\s*.*$',
        r'^GÃ¼ncelleme\s*[:=]\s*.*$',
        r'^Update\s*[:=]\s*.*$',
    ]
    
    lines = text.split('\n')
    cleaned_lines = []
    
    for line in lines:
        is_metadata = False
        for pattern in metadata_patterns:
            if re.match(pattern, line.strip(), re.IGNORECASE):
                is_metadata = True
                break
        
        if not is_metadata:
            cleaned_lines.append(line)
    
    return '\n'.join(cleaned_lines)


def remove_social_media_artifacts(text: str) -> str:
    if not text:
        return ""
    
    text = re.sub(r'https?://t\.co/\w+', '', text)
    
    text = re.sub(r'https?://(?:www\.)?instagram\.com/\S+', '', text)
    
    text = re.sub(r'https?://(?:www\.)?facebook\.com/\S+', '', text)
    
    text = re.sub(r'â€”\s*@\w+\s*\([^)]+\)', '', text)
    
    text = re.sub(r'@\w+', '', text)
    
    text = re.sub(r'#[\wÄŸÃ¼ÅŸÄ±Ã¶Ã§ÄžÃœÅžÄ°Ã–Ã‡]+', '', text)
    
    text = re.sub(r'Bu iÃ§erik \w+ alÄ±nmÄ±ÅŸtÄ±r\.?', '', text, flags=re.IGNORECASE)
    
    return text


def enhanced_clean_pipeline(text: str) -> str:
    if not text:
        return ""
    
    text = remove_social_media_artifacts(text)
    text = remove_metadata_lines(text)
    text = remove_clickbait_phrases(text)
    text = fix_all_caps_text(text)
    text = normalize_whitespace(text)
    text = fix_punctuation_spacing(text)
    
    return text


def test_cleaning_functions():
    print("ðŸ§ª TEMÄ°ZLEME FONKSÄ°YONLARI TEST EDÄ°LÄ°YOR...\n")
    
    test1 = "FOSÄ°L KARABORSASI VAR EMEKLI PROFESÃ–R AÃ‡IKLADI"
    print(f"Test 1 - BÃ¼yÃ¼k Harf DÃ¼zeltme:")
    print(f"Ã–nce:  {test1}")
    print(f"Sonra: {fix_all_caps_text(test1)}\n")
    
    test2 = "ÅžOKA UÄžRAYACAKSINIZ! Bu haber sosyal medyayÄ± salladÄ± TIKLAYIN!"
    print(f"Test 2 - Clickbait Temizleme:")
    print(f"Ã–nce:  {test2}")
    print(f"Sonra: {remove_clickbait_phrases(test2)}\n")
    
    test3 = "Harika bir geliÅŸme #teknoloji @johnDoe https://t.co/abc123 â€” @user (01.12.2025)"
    print(f"Test 3 - Sosyal Medya Temizleme:")
    print(f"Ã–nce:  {test3}")
    print(f"Sonra: {remove_social_media_artifacts(test3)}\n")
    
    test4 = "Merhaba , nasÄ±lsÄ±n ?Ben iyiyim,teÅŸekkÃ¼rler !"
    print(f"Test 4 - Noktalama DÃ¼zeltme:")
    print(f"Ã–nce:  {test4}")
    print(f"Sonra: {fix_punctuation_spacing(test4)}\n")
    
    test5 = """FOSÄ°L KARABORSASI VAR EMEKLI PROFESÃ–R AÃ‡IKLADI
    
    ÅžOKA UÄžRAYACAKSINIZ! Bu haber    Ã§ok Ã¶nemli  ,  dikkat  !
    
    Kaynak: Reuters
    EditÃ¶r: Ahmet YÄ±lmaz
    
    https://t.co/abc123 @user #haber
    """
    print(f"Test 5 - Tam Pipeline:")
    print(f"Ã–nce:\n{test5}")
    print(f"\nSonra:\n{enhanced_clean_pipeline(test5)}\n")


if __name__ == "__main__":
    test_cleaning_functions()
