from rapidfuzz import fuzz
from datetime import datetime, timedelta
from typing import List, Dict, Optional
import logging
from config import Config

logger = logging.getLogger(__name__)

# ------------------------------------------
# Ayarlar (Config'den alÄ±nÄ±r)
# ------------------------------------------
SIMILARITY_THRESHOLD = Config.SIMILARITY_THRESHOLD  # %85
TIME_DIFF_THRESHOLD = Config.TIME_DIFF_THRESHOLD    # 15 dakika (900 saniye)

# ------------------------------------------
# 1) BaÅŸlÄ±k BenzerliÄŸi (GeliÅŸtirilmiÅŸ)
# ------------------------------------------
def titles_similar(t1: str, t2: str, threshold: int = None) -> bool:
    """
    Ä°ki baÅŸlÄ±ÄŸÄ±n benzer olup olmadÄ±ÄŸÄ±nÄ± kontrol eder.
    
    Args:
        t1: Ä°lk baÅŸlÄ±k
        t2: Ä°kinci baÅŸlÄ±k
        threshold: Benzerlik eÅŸiÄŸi (default: config'den)
    
    Returns:
        bool: Benzerlik eÅŸiÄŸi aÅŸÄ±lÄ±ysa True
    """
    if not t1 or not t2:
        return False
    
    if threshold is None:
        threshold = SIMILARITY_THRESHOLD
    
    # Normalizasyon (kÃ¼Ã§Ã¼k harf + whitespace temizleme)
    t1_normalized = " ".join(t1.lower().split())
    t2_normalized = " ".join(t2.lower().split())
    
    # Tam eÅŸleÅŸme kontrolÃ¼
    if t1_normalized == t2_normalized:
        return True
    
    # Fuzzy similarity
    similarity = fuzz.ratio(t1_normalized, t2_normalized)
    
    logger.debug(f"ğŸ“Š BaÅŸlÄ±k benzerliÄŸi: {similarity}% | {threshold}%")
    
    return similarity >= threshold


# ------------------------------------------
# 2) URL BenzerliÄŸi (GeliÅŸtirilmiÅŸ)
# ------------------------------------------
def urls_similar(u1: str, u2: str) -> bool:
    """
    Ä°ki URL'nin aynÄ± habere iÅŸaret edip etmediÄŸini kontrol eder.
    
    Normalizasyon:
    - http/https farkÄ±nÄ± yok sayar
    - www. farkÄ±nÄ± yok sayar
    - Query string'leri (?ref=...) atar
    - Fragment'leri (#section) atar
    - Trailing slash'leri atar
    
    Args:
        u1: Ä°lk URL
        u2: Ä°kinci URL
    
    Returns:
        bool: AynÄ± URL ise True
    """
    if not u1 or not u2:
        return False
    
    def normalize(url: str) -> str:
        """URL'i normalize et"""
        normalized = (
            url.lower()
            .replace("https://", "")
            .replace("http://", "")
            .replace("www.", "")
            .split("?")[0]  # Query string at
            .split("#")[0]  # Fragment at
            .rstrip("/")    # Trailing slash at
        )
        return normalized
    
    normalized_u1 = normalize(u1)
    normalized_u2 = normalize(u2)
    
    # Tam eÅŸleÅŸme
    if normalized_u1 == normalized_u2:
        logger.debug(f"ğŸ”— URL eÅŸleÅŸti: {normalized_u1}")
        return True
    
    # Subdomain farkÄ± varsa bile ana URL aynÄ± mÄ±?
    # Ã–rnek: m.example.com/news/123 vs example.com/news/123
    def get_path(url: str) -> str:
        """Sadece path kÄ±smÄ±nÄ± al (domain'siz)"""
        parts = url.split("/", 1)
        return parts[1] if len(parts) > 1 else ""
    
    path1 = get_path(normalized_u1)
    path2 = get_path(normalized_u2)
    
    if path1 and path2 and path1 == path2:
        logger.debug(f"ğŸ”— URL path eÅŸleÅŸti: /{path1}")
        return True
    
    return False


# ------------------------------------------
# 3) Tarih YakÄ±nlÄ±ÄŸÄ± (GeliÅŸtirilmiÅŸ)
# ------------------------------------------
def dates_close(d1: Optional[str], d2: Optional[str], threshold: int = None) -> bool:
    """
    Ä°ki yayÄ±n tarihinin birbirine yakÄ±n olup olmadÄ±ÄŸÄ±nÄ± kontrol eder.
    
    Args:
        d1: Ä°lk tarih (ISO format veya timestamp)
        d2: Ä°kinci tarih (ISO format veya timestamp)
        threshold: Zaman farkÄ± eÅŸiÄŸi (saniye, default: config'den)
    
    Returns:
        bool: Tarihler yakÄ±nsa True
    """
    if not d1 or not d2:
        return False
    
    if threshold is None:
        threshold = TIME_DIFF_THRESHOLD
    
    try:
        # ISO format parse etmeyi dene
        dt1 = datetime.fromisoformat(d1.replace("Z", "+00:00"))
        dt2 = datetime.fromisoformat(d2.replace("Z", "+00:00"))
        
    except (ValueError, AttributeError):
        # ISO format deÄŸilse, farklÄ± formatlarda dene
        try:
            from dateutil import parser
            dt1 = parser.parse(d1)
            dt2 = parser.parse(d2)
        except:
            logger.debug(f"âš ï¸  Tarih parse edilemedi: {d1} / {d2}")
            return False
    
    # Zaman farkÄ±nÄ± hesapla
    diff_seconds = abs((dt1 - dt2).total_seconds())
    
    logger.debug(f"â° Tarih farkÄ±: {diff_seconds}s | EÅŸik: {threshold}s")
    
    return diff_seconds <= threshold


# ------------------------------------------
# 4) TEKÄ°LLEÅTÄ°RME (Ana Fonksiyon)
# ------------------------------------------
def remove_duplicates(news_list: List[Dict]) -> List[Dict]:
    """
    Haber listesinde duplicate haberleri temizler.
    
    Duplicate kriterleri (herhangi biri saÄŸlanÄ±rsa duplicate):
    1. BaÅŸlÄ±klar %85+ benzer
    2. URL'ler aynÄ±
    3. YayÄ±n zamanlarÄ± Â±15 dakika iÃ§inde VE baÅŸlÄ±klar %70+ benzer
    
    Args:
        news_list: Haber listesi
    
    Returns:
        Duplicate'siz haber listesi
    """
    if not news_list:
        return []
    
    unique = []
    duplicates = []
    
    for article in news_list:
        duplicate_found = False
        duplicate_reason = None
        
        for existing in unique:
            # Kriter 1: BaÅŸlÄ±k benzerliÄŸi
            if titles_similar(article.get("title", ""), existing.get("title", "")):
                duplicate_found = True
                duplicate_reason = "baÅŸlÄ±k_benzerliÄŸi"
                break
            
            # Kriter 2: URL aynÄ±lÄ±ÄŸÄ±
            if urls_similar(article.get("url", ""), existing.get("url", "")):
                duplicate_found = True
                duplicate_reason = "url_aynÄ±"
                break
            
            # Kriter 3: Tarih yakÄ±nlÄ±ÄŸÄ± + baÅŸlÄ±k %70 benzer
            if dates_close(
                article.get("publishedAt", ""),
                existing.get("publishedAt", "")
            ) and titles_similar(
                article.get("title", ""),
                existing.get("title", ""),
                threshold=70  # Daha dÃ¼ÅŸÃ¼k eÅŸik
            ):
                duplicate_found = True
                duplicate_reason = "zaman_ve_baÅŸlÄ±k"
                break
        
        if duplicate_found:
            duplicates.append(article)
            logger.debug(f"â­ï¸  Duplicate atlandÄ± ({duplicate_reason}): {article.get('title', '')[:50]}...")
        else:
            unique.append(article)
            logger.debug(f"âœ… Benzersiz haber: {article.get('title', '')[:50]}...")
    
    # Ä°statistik logla
    if duplicates:
        logger.info(f"ğŸ§¹ Duplicate temizleme: "
                   f"{len(news_list)} â†’ {len(unique)} haber "
                   f"({len(duplicates)} duplicate atÄ±ldÄ±)")
    else:
        logger.info(f"âœ… Duplicate yok: {len(unique)} benzersiz haber")
    
    return unique


# ------------------------------------------
# 5) Ä°ki Liste ArasÄ±nda Duplicate KontrolÃ¼
# ------------------------------------------
def filter_against_existing(new_articles: List[Dict], existing_articles: List[Dict]) -> List[Dict]:
    """
    Yeni haberleri, mevcut haberlerle karÅŸÄ±laÅŸtÄ±rÄ±p duplicate olanlarÄ± filtreler.
    
    KullanÄ±m: DB'ye kaydetmeden Ã¶nce yeni haberlerin duplicate olup olmadÄ±ÄŸÄ±nÄ± kontrol et
    
    Args:
        new_articles: Yeni Ã§ekilen haberler
        existing_articles: DB'de zaten var olan haberler
    
    Returns:
        Sadece gerÃ§ekten yeni olan haberler
    """
    if not existing_articles:
        return new_articles
    
    unique_new = []
    
    for new in new_articles:
        is_duplicate = False
        
        for existing in existing_articles:
            if (
                titles_similar(new.get("title", ""), existing.get("title", "")) or
                urls_similar(new.get("url", ""), existing.get("url", ""))
            ):
                is_duplicate = True
                logger.debug(f"â­ï¸  Zaten var: {new.get('title', '')[:50]}...")
                break
        
        if not is_duplicate:
            unique_new.append(new)
    
    logger.info(f"ğŸ“Š Yeni haberler: {len(new_articles)} â†’ {len(unique_new)} gerÃ§ekten yeni")
    
    return unique_new


# ------------------------------------------
# 6) Duplicate Ä°statistikleri
# ------------------------------------------
def get_duplicate_stats(news_list: List[Dict]) -> Dict:
    """
    Haber listesindeki duplicate oranÄ±nÄ± hesaplar (analiz iÃ§in)
    
    Returns:
        {
            "total": int,
            "unique": int,
            "duplicates": int,
            "duplicate_rate": float
        }
    """
    total = len(news_list)
    unique = len(remove_duplicates(news_list))
    duplicates = total - unique
    
    return {
        "total": total,
        "unique": unique,
        "duplicates": duplicates,
        "duplicate_rate": round((duplicates / total * 100), 2) if total > 0 else 0
    }


# ------------------------------------------
# 7) Haber Kalitesi Skorlama (Bonus)
# ------------------------------------------
def calculate_quality_score(article: Dict) -> int:
    """
    Haberin kalitesini skorlar (0-100)
    
    Kriterler:
    - BaÅŸlÄ±k var mÄ±? (+20)
    - Description var mÄ±? (+20)
    - Image var mÄ±? (+20)
    - URL var mÄ±? (+20)
    - Tarih var mÄ±? (+20)
    
    Returns:
        Kalite skoru (0-100)
    """
    score = 0
    
    if article.get("title") and len(article["title"]) > 10:
        score += 20
    
    if article.get("description") and len(article["description"]) > 20:
        score += 20
    
    if article.get("image"):
        score += 20
    
    if article.get("url"):
        score += 20
    
    if article.get("publishedAt"):
        score += 20
    
    return score


def filter_low_quality(news_list: List[Dict], min_score: int = 60) -> List[Dict]:
    """
    DÃ¼ÅŸÃ¼k kaliteli haberleri filtreler
    
    Args:
        news_list: Haber listesi
        min_score: Minimum kalite skoru (default: 60)
    
    Returns:
        Sadece yÃ¼ksek kaliteli haberler
    """
    filtered = []
    
    for article in news_list:
        score = calculate_quality_score(article)
        
        if score >= min_score:
            filtered.append(article)
            logger.debug(f"âœ… Kalite OK ({score}): {article.get('title', '')[:50]}...")
        else:
            logger.debug(f"âŒ DÃ¼ÅŸÃ¼k kalite ({score}): {article.get('title', '')[:50]}...")
    
    logger.info(f"ğŸ¯ Kalite filtresi: {len(news_list)} â†’ {len(filtered)} haber")
    
    return filtered
