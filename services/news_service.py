import feedparser
from datetime import datetime
from models.db import get_db, put_db
import requests
from bs4 import BeautifulSoup
import re
from urllib.parse import urlparse, urljoin

# ===============================
# Ã‡ALIÅAN + YENÄ° PREMIUM RSS KAYNAKLARI
# ===============================
RSS_FEEDS = {
    # GENEL HABER - YÃ¼ksek Kalite (Ã‡ALIÅAN)
    "BBC TÃ¼rkÃ§e": "https://www.bbc.com/turkce/index.xml",
    "HabertÃ¼rk": "https://www.haberturk.com/rss",
    "CNN TÃ¼rk": "https://www.cnnturk.com/feed/rss/news",
    "SÃ¶zcÃ¼": "https://www.sozcu.com.tr/rss/",
    "HÃ¼rriyet": "https://www.hurriyet.com.tr/rss/anasayfa",
    "Milliyet GÃ¼ndem": "https://www.milliyet.com.tr/rss/rssnew/gundemrss.xml",
    
    # YENÄ° EKLENDÄ° - YÃœKSEK GÃ–RSELLÄ°
    "Cumhuriyet": "https://www.cumhuriyet.com.tr/rss/son_dakika.xml",
    "Sabah": "https://www.sabah.com.tr/rss/anasayfa.xml",
    "Posta": "https://www.posta.com.tr/rss/anasayfa.xml",
    "Yeni Åafak": "https://www.yenisafak.com/Rss",
    "Star": "https://www.star.com.tr/rss.xml",
    
    # EKONOMÄ° - GÃ¶rsel AÄŸÄ±rlÄ±klÄ± (Ã‡ALIÅAN)
    "Bloomberg HT": "https://www.bloomberght.com/rss",
    "Para Analiz": "https://www.paraanaliz.com/feed/",
    
    # YENÄ° EKONOMÄ°
    "Ekonomim": "https://www.ekonomim.com/rss/news.xml",
    "DÃ¼nya Gazetesi": "https://www.dunya.com/service/rss.php",
    
    # SPOR - YENÄ° KAYNAKLAR
    "Sporx": "https://www.sporx.com/rss",
    "A Spor": "https://www.aspor.com.tr/rss",
    "Haber Spor": "https://www.haberspor.com/rss",
    
    # TEKNOLOJÄ° - Premium GÃ¶rseller (Ã‡ALIÅAN)
    "WebTekno": "https://www.webtekno.com/rss.xml",
    "ShiftDelete": "https://shiftdelete.net/feed",
    
    # YENÄ° TEKNOLOJÄ°
    "TeknoLog": "https://teknolog.com/feed/",
    "Teknolojioku": "https://www.teknolojioku.com/feed/",
    
    # DÃœNYA HABERLERÄ°
    "Euronews TÃ¼rkÃ§e": "https://tr.euronews.com/rss",
    "DW TÃ¼rkÃ§e": "https://www.dw.com/rss/rss-tur-all/rss.xml",
    
    # YAÅAM & SAÄLIK
    "Mynet YaÅŸam": "https://www.mynet.com/rss/yasam",
    "WebMD (TR)": "https://www.webmd.com/rss/rss.aspx?RSSSource=RSS_PUBLIC",
}

# ===============================
# AKILLI GÃ–RSEL Ã‡EKME SÄ°STEMÄ°
# ===============================
def extract_high_quality_image(entry, feed_url, source_name):
    """
    Ã‡oklu yÃ¶ntemle yÃ¼ksek kaliteli gÃ¶rsel bul
    """
    image_url = ""
    
    # 1. RSS Media Tags (En HÄ±zlÄ± ve GÃ¼venilir)
    if hasattr(entry, "media_content") and entry.media_content:
        # En bÃ¼yÃ¼k Ã§Ã¶zÃ¼nÃ¼rlÃ¼klÃ¼ gÃ¶rseli seÃ§
        images = [m for m in entry.media_content if "image" in m.get("type", "")]
        if images:
            # Width'e gÃ¶re sÄ±rala
            images.sort(key=lambda x: int(x.get("width", 0)), reverse=True)
            image_url = images[0].get("url", "")
    
    if not image_url and hasattr(entry, "media_thumbnail") and entry.media_thumbnail:
        thumbnails = entry.media_thumbnail
        if thumbnails:
            # En bÃ¼yÃ¼k thumbnail'i al
            thumbnails.sort(key=lambda x: int(x.get("width", 0)), reverse=True)
            image_url = thumbnails[0].get("url", "")
    
    # 2. Enclosures (Podcast/Video gÃ¶rselleri)
    if not image_url and hasattr(entry, "enclosures") and entry.enclosures:
        for enc in entry.enclosures:
            if "image" in enc.get("type", "").lower():
                image_url = enc.get("href", "")
                break
    
    # 3. Ä°Ã§erik iÃ§inden gÃ¶rsel ara (HTML parsing)
    if not image_url:
        content = entry.get("summary", "") or entry.get("description", "") or entry.get("content", [{}])[0].get("value", "")
        
        # img src bul
        img_matches = re.findall(r'<img[^>]+src=["\']([^"\']+)["\']', content, re.IGNORECASE)
        if img_matches:
            # Ä°lk bÃ¼yÃ¼k gÃ¶rseli al (data:image hariÃ§)
            for img in img_matches:
                if not img.startswith("data:"):
                    image_url = img
                    break
    
    # 4. URL DÃ¼zeltmeleri
    if image_url:
        # Protocol ekle
        if image_url.startswith("//"):
            image_url = "https:" + image_url
        # Relative URL'leri absolute yap
        elif image_url.startswith("/"):
            parsed = urlparse(feed_url)
            image_url = f"{parsed.scheme}://{parsed.netloc}{image_url}"
        # Query parametrelerini temizle (bazÄ± siteler boyut parametresi ekler)
        if "?" in image_url and any(x in image_url for x in ["w=", "h=", "size="]):
            base_url = image_url.split("?")[0]
            # EÄŸer dÃ¼ÅŸÃ¼k Ã§Ã¶zÃ¼nÃ¼rlÃ¼k parametresi varsa kaldÄ±r
            if any(x in image_url for x in ["w=100", "w=200", "w=300", "thumb", "small"]):
                image_url = base_url
    
    # 5. Kalite KontrolÃ¼
    if image_url:
        # Ã‡ok kÃ¼Ã§Ã¼k gÃ¶rselleri reddet
        if any(x in image_url.lower() for x in ["1x1", "pixel", "tracking", "beacon"]):
            return ""
        
        # Data URI'leri reddet
        if image_url.startswith("data:"):
            return ""
        
        # Sosyal medya ikonlarÄ±nÄ± reddet
        if any(x in image_url.lower() for x in ["facebook", "twitter", "instagram", "logo", "icon"]):
            return ""
    
    return image_url

# ===============================
# HABERÄ° NORMALIZE ET
# ===============================
def normalize(entry, source_name, feed_url):
    """
    Haber verisini temizle ve dÃ¼zenle
    """
    title = entry.get("title", "").strip()
    
    # AÃ§Ä±klama
    description = entry.get("summary", "") or entry.get("description", "")
    if isinstance(description, list):
        description = description[0].get("value", "") if description else ""
    
    # HTML etiketlerini temizle
    description = re.sub(r'<[^>]+>', '', description)
    description = re.sub(r'\s+', ' ', description).strip()
    
    # UzunluÄŸu sÄ±nÄ±rla
    if len(description) > 250:
        description = description[:247] + "..."
    
    # Link
    link = entry.get("link", "").strip()
    
    # Tarih
    try:
        if hasattr(entry, "published_parsed") and entry.published_parsed:
            published_date = datetime(*entry.published_parsed[:6])
        else:
            published_date = datetime.now()
    except:
        published_date = datetime.now()
    
    # YÃ¼ksek kaliteli gÃ¶rsel
    image = extract_high_quality_image(entry, feed_url, source_name)
    
    return {
        "baslik": title,
        "aciklama": description,
        "url": link,
        "gorsel": image,
        "kaynak": source_name,
        "tarih": published_date,
        "has_image": bool(image)  # GÃ¶rsel var mÄ± kontrolÃ¼
    }

# ===============================
# RSS VERÄ°LERÄ°NÄ° Ã‡EK + KAYDET
# ===============================
def fetch_and_save_news():
    """
    TÃ¼m RSS kaynaklarÄ±ndan haberleri Ã§ek ve veritabanÄ±na kaydet
    """
    conn = get_db()
    cur = conn.cursor()
    
    new_count = 0
    total_with_images = 0
    failed_feeds = []
    
    print(f"\n{'='*60}")
    print(f"ğŸ“¡ {len(RSS_FEEDS)} RSS kaynaÄŸÄ±ndan haber Ã§ekiliyor...")
    print(f"{'='*60}\n")
    
    for source_name, feed_url in RSS_FEEDS.items():
        try:
            # RSS'i parse et
            feed = feedparser.parse(feed_url)
            
            # HatalÄ± RSS kontrolÃ¼
            if feed.bozo and not feed.entries:
                print(f"âŒ {source_name}: RSS hatasÄ±")
                failed_feeds.append(source_name)
                continue
            
            entries_count = len(feed.entries)
            print(f"ğŸ” {source_name}: {entries_count} haber bulundu")
            
            source_new = 0
            source_images = 0
            
            # Her kaynaktan max 8 haber al (performans iÃ§in)
            for entry in feed.entries[:8]:
                try:
                    item = normalize(entry, source_name, feed_url)
                    
                    # Temel validasyon
                    if not item["baslik"] or not item["url"]:
                        continue
                    
                    # BaÅŸlÄ±k Ã§ok kÄ±sa ise geÃ§
                    if len(item["baslik"]) < 10:
                        continue
                    
                    # URL duplicate kontrolÃ¼
                    cur.execute("SELECT id FROM haberler WHERE url = %s", (item["url"],))
                    if cur.fetchone():
                        continue
                    
                    # VeritabanÄ±na ekle
                    cur.execute("""
                        INSERT INTO haberler (baslik, aciklama, gorsel, kaynak, url, tarih)
                        VALUES (%s, %s, %s, %s, %s, %s)
                    """, (
                        item["baslik"],
                        item["aciklama"],
                        item["gorsel"],
                        item["kaynak"],
                        item["url"],
                        item["tarih"]
                    ))
                    
                    source_new += 1
                    new_count += 1
                    
                    if item["has_image"]:
                        source_images += 1
                        total_with_images += 1
                
                except Exception as e:
                    print(f"âš ï¸ Haber iÅŸleme hatasÄ± ({source_name}): {e}")
                    continue
            
            if source_new > 0:
                print(f"   âœ… {source_new} yeni haber ({source_images} gÃ¶rselli)")
        
        except Exception as e:
            print(f"âŒ {source_name}: {str(e)[:50]}")
            failed_feeds.append(source_name)
    
    # Eski haberleri temizle (3 gÃ¼nden eski)
    cur.execute("""
        DELETE FROM haberler 
        WHERE tarih < NOW() - INTERVAL '3 days';
    """)
    deleted_count = cur.rowcount
    
    # DeÄŸiÅŸiklikleri kaydet
    conn.commit()
    put_db(conn)
    
    # Ã–zet rapor
    print(f"\n{'='*60}")
    print(f"ğŸ“Š Ã–ZET:")
    print(f"   âœ… {new_count} yeni haber eklendi")
    print(f"   ğŸ–¼ï¸  {total_with_images} haberde gÃ¶rsel var ({int(total_with_images/new_count*100) if new_count > 0 else 0}%)")
    print(f"   ğŸ§¹ {deleted_count} eski haber silindi")
    if failed_feeds:
        print(f"   âš ï¸  {len(failed_feeds)} kaynak baÅŸarÄ±sÄ±z: {', '.join(failed_feeds[:3])}")
    print(f"{'='*60}\n")
    
    return new_count
