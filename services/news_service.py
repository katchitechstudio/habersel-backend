import feedparser
from datetime import datetime
from models.db import get_db, put_db
import requests
from bs4 import BeautifulSoup
import re
from urllib.parse import urlparse, urljoin

# ===============================
# Ã‡ALIÅAN RSS KAYNAKLARI + GZT
# ===============================
RSS_FEEDS = {
    # GZT - YENÄ° EKLENEN (YÃ¼ksek Kalite)
    "GZT Genel": "https://www.gzt.com/rss",
    "GZT GÃ¼ndem": "https://www.gzt.com/rss/gundem",
    "GZT DÃ¼nya": "https://www.gzt.com/rss/dunya",
    "GZT Teknoloji": "https://www.gzt.com/rss/teknoloji",
    "GZT Spor": "https://www.gzt.com/rss/spor",
    
    # GENEL HABER - YÃ¼ksek Kalite (Ã‡ALIÅAN)
    "BBC TÃ¼rkÃ§e": "https://www.bbc.com/turkce/index.xml",
    "HabertÃ¼rk": "https://www.haberturk.com/rss",
    "CNN TÃ¼rk": "https://www.cnnturk.com/feed/rss/news",
    "SÃ¶zcÃ¼": "https://www.sozcu.com.tr/rss/",
    "HÃ¼rriyet": "https://www.hurriyet.com.tr/rss/anasayfa",
    "Cumhuriyet": "https://www.cumhuriyet.com.tr/rss/son_dakika.xml",
    "Sabah": "https://www.sabah.com.tr/rss/anasayfa.xml",
    "Posta": "https://www.posta.com.tr/rss/anasayfa.xml",
    "Yeni Åafak": "https://www.yenisafak.com/Rss",
    
    # EKONOMÄ°
    "Bloomberg HT": "https://www.bloomberght.com/rss",
    "Para Analiz": "https://www.paraanaliz.com/feed/",
    
    # TEKNOLOJÄ°
    "WebTekno": "https://www.webtekno.com/rss.xml",
    "ShiftDelete": "https://shiftdelete.net/feed",
    "TeknoLog": "https://teknolog.com/feed/",
    
    # DÃœNYA
    "Euronews TÃ¼rkÃ§e": "https://tr.euronews.com/rss",
}

# ===============================
# AKILLI GÃ–RSEL Ã‡EKME SÄ°STEMÄ°
# ===============================
def extract_high_quality_image(entry, feed_url, source_name):
    """
    Ã‡oklu yÃ¶ntemle yÃ¼ksek kaliteli gÃ¶rsel bul
    """
    image_url = ""
    
    # 1. RSS Media Tags
    if hasattr(entry, "media_content") and entry.media_content:
        images = [m for m in entry.media_content if "image" in m.get("type", "")]
        if images:
            images.sort(key=lambda x: int(x.get("width", 0)), reverse=True)
            image_url = images[0].get("url", "")
    
    if not image_url and hasattr(entry, "media_thumbnail") and entry.media_thumbnail:
        thumbnails = entry.media_thumbnail
        if thumbnails:
            thumbnails.sort(key=lambda x: int(x.get("width", 0)), reverse=True)
            image_url = thumbnails[0].get("url", "")
    
    # 2. Enclosures
    if not image_url and hasattr(entry, "enclosures") and entry.enclosures:
        for enc in entry.enclosures:
            if "image" in enc.get("type", "").lower():
                image_url = enc.get("href", "")
                break
    
    # 3. Ä°Ã§erik iÃ§inden gÃ¶rsel ara
    if not image_url:
        content = entry.get("summary", "") or entry.get("description", "")
        img_matches = re.findall(r'<img[^>]+src=["\']([^"\']+)["\']', content, re.IGNORECASE)
        if img_matches:
            for img in img_matches:
                if not img.startswith("data:"):
                    image_url = img
                    break
    
    # 4. URL DÃ¼zeltmeleri
    if image_url:
        if image_url.startswith("//"):
            image_url = "https:" + image_url
        elif image_url.startswith("/"):
            parsed = urlparse(feed_url)
            image_url = f"{parsed.scheme}://{parsed.netloc}{image_url}"
    
    # 5. Kalite KontrolÃ¼
    if image_url:
        if any(x in image_url.lower() for x in ["1x1", "pixel", "tracking", "beacon", "logo", "icon"]):
            return ""
        if image_url.startswith("data:"):
            return ""
    
    return image_url

# ===============================
# HABERÄ° NORMALIZE ET
# ===============================
def normalize(entry, source_name, feed_url):
    """
    Haber verisini temizle ve dÃ¼zenle
    """
    title = entry.get("title", "").strip()
    
    # AÃ§Ä±klama
    description = entry.get("summary", "") or entry.get("description", "")
    if isinstance(description, list):
        description = description[0].get("value", "") if description else ""
    
    # HTML etiketlerini temizle
    description = re.sub(r'<[^>]+>', '', description)
    description = re.sub(r'\s+', ' ', description).strip()
    
    # UzunluÄŸu sÄ±nÄ±rla
    if len(description) > 250:
        description = description[:247] + "..."
    
    # Link
    link = entry.get("link", "").strip()
    
    # Tarih
    try:
        if hasattr(entry, "published_parsed") and entry.published_parsed:
            published_date = datetime(*entry.published_parsed[:6])
        else:
            published_date = datetime.now()
    except:
        published_date = datetime.now()
    
    # YÃ¼ksek kaliteli gÃ¶rsel
    image = extract_high_quality_image(entry, feed_url, source_name)
    
    return {
        "baslik": title,
        "aciklama": description,
        "url": link,
        "gorsel": image,
        "kaynak": source_name,
        "tarih": published_date,
        "has_image": bool(image)
    }

# ===============================
# RSS VERÄ°LERÄ°NÄ° Ã‡EK + KAYDET (DÃœZELTÄ°LMÄ°Å)
# ===============================
def fetch_and_save_news():
    """
    TÃ¼m RSS kaynaklarÄ±ndan haberleri Ã§ek ve veritabanÄ±na kaydet
    """
    conn = get_db()
    cur = conn.cursor()
    
    new_count = 0
    total_with_images = 0
    failed_feeds = []
    
    print(f"\n{'='*60}")
    print(f"ğŸ“¡ {len(RSS_FEEDS)} RSS kaynaÄŸÄ±ndan haber Ã§ekiliyor...")
    print(f"{'='*60}\n")
    
    for source_name, feed_url in RSS_FEEDS.items():
        try:
            # RSS'i parse et
            feed = feedparser.parse(feed_url)
            
            # HatalÄ± RSS kontrolÃ¼
            if feed.bozo and not feed.entries:
                print(f"âŒ {source_name}: RSS hatasÄ±")
                failed_feeds.append(source_name)
                continue
            
            entries_count = len(feed.entries)
            print(f"ğŸ” {source_name}: {entries_count} haber bulundu")
            
            source_new = 0
            source_images = 0
            
            # Her kaynaktan max 8 haber al
            for entry in feed.entries[:8]:
                try:
                    item = normalize(entry, source_name, feed_url)
                    
                    # Temel validasyon
                    if not item["baslik"] or not item["url"]:
                        continue
                    
                    # BaÅŸlÄ±k Ã§ok kÄ±sa ise geÃ§
                    if len(item["baslik"]) < 10:
                        continue
                    
                    # ğŸ”¥ DÃœZELTME: URL duplicate kontrolÃ¼
                    cur.execute("SELECT id FROM haberler WHERE url = %s", (item["url"],))
                    if cur.fetchone():
                        continue
                    
                    # ğŸ”¥ DÃœZELTME: VeritabanÄ±na ekle - Her insert kendi transaction'Ä±nda
                    try:
                        cur.execute("""
                            INSERT INTO haberler (baslik, aciklama, gorsel, kaynak, url, tarih)
                            VALUES (%s, %s, %s, %s, %s, %s)
                            ON CONFLICT (url) DO NOTHING
                        """, (
                            item["baslik"],
                            item["aciklama"],
                            item["gorsel"],
                            item["kaynak"],
                            item["url"],
                            item["tarih"]
                        ))
                        
                        # ğŸ”¥ DÃœZELTME: Her insert'ten sonra commit
                        conn.commit()
                        
                        source_new += 1
                        new_count += 1
                        
                        if item["has_image"]:
                            source_images += 1
                            total_with_images += 1
                    
                    except Exception as insert_error:
                        # ğŸ”¥ DÃœZELTME: Hata olursa rollback yap
                        conn.rollback()
                        print(f"âš ï¸ Insert hatasÄ± ({source_name}): {str(insert_error)[:60]}")
                        continue
                
                except Exception as e:
                    conn.rollback()
                    print(f"âš ï¸ Haber iÅŸleme hatasÄ± ({source_name}): {str(e)[:60]}")
                    continue
            
            if source_new > 0:
                print(f"   âœ… {source_new} yeni haber ({source_images} gÃ¶rselli)")
        
        except Exception as e:
            print(f"âŒ {source_name}: {str(e)[:50]}")
            failed_feeds.append(source_name)
    
    # ğŸ”¥ DÃœZELTME: Eski haberleri temizle - AyrÄ± transaction
    try:
        cur.execute("""
            DELETE FROM haberler 
            WHERE tarih < NOW() - INTERVAL '3 days';
        """)
        deleted_count = cur.rowcount
        conn.commit()
    except Exception as e:
        conn.rollback()
        print(f"âš ï¸ Temizlik hatasÄ±: {e}")
        deleted_count = 0
    
    # Connection'Ä± kapat
    put_db(conn)
    
    # Ã–zet rapor
    print(f"\n{'='*60}")
    print(f"ğŸ“Š Ã–ZET:")
    print(f"   âœ… {new_count} yeni haber eklendi")
    print(f"   ğŸ–¼ï¸  {total_with_images} haberde gÃ¶rsel var ({int(total_with_images/new_count*100) if new_count > 0 else 0}%)")
    print(f"   ğŸ§¹ {deleted_count} eski haber silindi")
    if failed_feeds:
        print(f"   âš ï¸  {len(failed_feeds)} kaynak baÅŸarÄ±sÄ±z: {', '.join(failed_feeds[:3])}")
    print(f"{'='*60}\n")
    
    return new_count
