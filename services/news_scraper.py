from newspaper import Article
from models.news_models import NewsModel
from utils.helpers import full_clean_news_pipeline  # üÜï TEMƒ∞ZLEME FONKSƒ∞YONU
import time
import random
import logging
import threading

logger = logging.getLogger(__name__)

USER_AGENTS = [
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
    'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:121.0) Gecko/20100101 Firefox/121.0',
    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:121.0) Gecko/20100101 Firefox/121.0'
]


def scrape_article_content(url: str):
    """
    Haber URL'sinden tam i√ßeriƒüi √ßeker
    
    Args:
        url: Haber URL'si
    
    Returns:
        tuple: (full_text, scraped_image)
    """
    try:
        user_agent = random.choice(USER_AGENTS)
        
        article = Article(url, language='tr')
        article.config.browser_user_agent = user_agent
        article.config.request_timeout = 15
        
        article.download()
        article.parse()
        
        full_text = article.text.strip()
        
        if len(full_text) < 100:
            logger.warning(f"‚ö†Ô∏è √áok kƒ±sa i√ßerik: {len(full_text)} karakter")
            return None, None
        
        scraped_image = article.top_image if article.top_image else None
        
        logger.debug(f"‚úÖ {len(full_text)} karakter √ßekildi")
        if scraped_image:
            logger.debug(f"üñºÔ∏è G√∂rsel bulundu: {scraped_image}")
        
        return full_text, scraped_image
        
    except Exception as e:
        logger.error(f"‚ùå Scrape hatasƒ±: {e}")
        return None, None


def scrape_latest_news(count=15):
    """
    En son scrape edilmemi≈ü haberleri √ßeker ve veritabanƒ±na kaydeder
    
    üÜï YENƒ∞: ƒ∞√ßerikleri temizleyerek kaydeder
    
    Args:
        count: √áekilecek haber sayƒ±sƒ±
    """
    logger.info(f"üîç Scrape edilecek haberler aranƒ±yor... (Hedef: {count})")
    
    pending_articles = NewsModel.get_unscraped(limit=count, exclude_blacklist=True)
    
    if not pending_articles:
        logger.info("‚úÖ Scrape edilecek haber yok")
        return
    
    logger.info(f"üì∞ {len(pending_articles)} haber scrape edilecek...")
    
    success = 0
    failed = 0
    
    for idx, article in enumerate(pending_articles, 1):
        try:
            article_url = article['url']
            article_id = article['id']
            article_title = article.get('title', '')
            article_date = article.get('published')
            
            # Blacklist kontrol√º
            if NewsModel.is_blacklisted(article_url, threshold=3):
                logger.debug(f"üö´ [{idx}/{len(pending_articles)}] Blacklist'te, atlanƒ±yor: {article_title[:60]}...")
                failed += 1
                continue
            
            logger.info(f"üîÑ [{idx}/{len(pending_articles)}] {article_title[:60]}...")
            
            # API'den gelen g√∂rsel
            api_image = article.get('image')
            
            # ƒ∞√ßeriƒüi scrape et
            full_content, scraped_image = scrape_article_content(article_url)
            
            if full_content:
                # üÜï ƒ∞√áERƒ∞ƒûƒ∞ TEMƒ∞ZLE
                logger.debug("üßπ ƒ∞√ßerik temizleniyor...")
                cleaned_data = full_clean_news_pipeline(
                    title=article_title,
                    content=full_content,
                    description=article.get('description'),
                    date=article_date
                )
                
                # Temizlenmi≈ü i√ßerik
                cleaned_content = cleaned_data['content']
                cleaned_title = cleaned_data['title']
                
                # G√∂rsel √∂nceliƒüi: Scraper > API
                final_image = scraped_image if scraped_image else api_image
                
                # Veritabanƒ±na kaydet (temizlenmi≈ü i√ßerikle)
                NewsModel.update_full_content(
                    article_id, 
                    cleaned_content,  # üÜï Temizlenmi≈ü i√ßerik
                    final_image
                )
                
                # ƒ∞steƒüe baƒülƒ±: Ba≈ülƒ±ƒüƒ± da g√ºncelle
                if cleaned_title and cleaned_title != article_title:
                    try:
                        NewsModel.update_title(article_id, cleaned_title)
                        logger.debug(f"üìù Ba≈ülƒ±k g√ºncellendi")
                    except:
                        pass  # Ba≈ülƒ±k g√ºncellemesi opsiyonel
                
                success += 1
                
                # ƒ∞statistikler
                original_char_count = len(full_content)
                cleaned_char_count = len(cleaned_content) if cleaned_content else 0
                cleaned_word_count = len(cleaned_content.split()) if cleaned_content else 0
                reduction_pct = round((1 - cleaned_char_count / original_char_count) * 100, 1) if original_char_count > 0 else 0
                
                if scraped_image:
                    logger.info(f"   ‚úÖ {cleaned_char_count} karakter (~{cleaned_word_count} kelime) [%{reduction_pct} temizlendi] (Scraper g√∂rseli)")
                elif api_image:
                    logger.info(f"   ‚úÖ {cleaned_char_count} karakter (~{cleaned_word_count} kelime) [%{reduction_pct} temizlendi] (API g√∂rseli)")
                else:
                    logger.info(f"   ‚úÖ {cleaned_char_count} karakter (~{cleaned_word_count} kelime) [%{reduction_pct} temizlendi] (g√∂rsel yok)")
            else:
                failed += 1
                NewsModel.add_to_blacklist(article_url, reason="content_extraction_failed")
                logger.warning(f"   ‚ö†Ô∏è ƒ∞√ßerik alƒ±namadƒ±")
            
            # Rate limiting - Son haberde bekleme
            if idx < len(pending_articles):
                wait_time = random.randint(25, 35)
                time.sleep(wait_time)
            
        except Exception as e:
            failed += 1
            NewsModel.add_to_blacklist(article['url'], reason=f"exception: {str(e)[:50]}")
            logger.error(f"   ‚ùå Hata: {e}")
    
    logger.info(f"üéâ Scraping tamamlandƒ±! Ba≈üarƒ±lƒ±: {success}, Ba≈üarƒ±sƒ±z: {failed}")


def scrape_all_pending_articles():
    """
    T√ºm bekleyen haberleri scrape eder (varsayƒ±lan 20 adet)
    """
    scrape_latest_news(count=20)


def scrape_in_background(count=15):
    """
    Scraping i≈ülemini arka planda ba≈ülatƒ±r
    
    Args:
        count: Scrape edilecek haber sayƒ±sƒ±
    """
    thread = threading.Thread(
        target=scrape_latest_news,
        args=(count,),
        daemon=True
    )
    thread.start()
    logger.info(f"üî• Scraping arka planda ba≈ülatƒ±ldƒ± ({count} haber)")
