from newspaper import Article, Config as NewspaperConfig
from models.news_models import NewsModel
from utils.helpers import full_clean_news_pipeline
import time
import random
import logging
import threading
import ssl
import requests
from bs4 import BeautifulSoup
import re

logger = logging.getLogger(__name__)

# ğŸ”¥ SSL HACK
try:
    _create_unverified_https_context = ssl._create_unverified_context
except AttributeError:
    pass
else:
    ssl._create_default_https_context = _create_unverified_https_context

# ğŸ¥¸ USER AGENT LÄ°STESÄ°
USER_AGENTS = [
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
    'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
]

def scrape_with_beautifulsoup(url: str) -> tuple:
    """
    BeautifulSoup ile manuel scraping (newspaper baÅŸarÄ±sÄ±z olursa)
    """
    try:
        headers = {
            'User-Agent': random.choice(USER_AGENTS),
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'tr-TR,tr;q=0.9,en-US;q=0.8,en;q=0.7',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
        }
        
        response = requests.get(url, headers=headers, timeout=20, verify=False)
        response.raise_for_status()
        response.encoding = 'utf-8'
        
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # ğŸ—‘ï¸ Gereksiz elementleri sil
        for tag in soup(['script', 'style', 'nav', 'header', 'footer', 'aside', 
                        'iframe', 'noscript', 'button', 'form']):
            tag.decompose()
        
        # ğŸ“° Ä°Ã§erik alanÄ±nÄ± bul (yaygÄ±n selector'lar)
        content_selectors = [
            'article',
            'div.article-content',
            'div.post-content',
            'div.entry-content',
            'div.content',
            'div.news-content',
            'div.detail-content',
            'div[itemprop="articleBody"]',
            'div.story-body',
            'div.article-body',
            'main',
        ]
        
        content_element = None
        for selector in content_selectors:
            content_element = soup.select_one(selector)
            if content_element:
                break
        
        # EÄŸer Ã¶zel selector bulamazsa tÃ¼m <p> tag'lerini topla
        if not content_element:
            content_element = soup
        
        # ğŸ“ TÃ¼m paragraflarÄ± topla
        paragraphs = []
        for p in content_element.find_all(['p', 'h2', 'h3', 'blockquote']):
            text = p.get_text(strip=True)
            # En az 30 karakter olan paragraflarÄ± al
            if len(text) >= 30:
                paragraphs.append(text)
        
        full_text = '\n\n'.join(paragraphs)
        
        # ğŸ–¼ï¸ Resim bul
        image_url = None
        img_selectors = [
            'meta[property="og:image"]',
            'article img',
            'div.article-content img',
            'div.post-content img',
        ]
        
        for selector in img_selectors:
            img_tag = soup.select_one(selector)
            if img_tag:
                image_url = img_tag.get('content') or img_tag.get('src')
                if image_url:
                    break
        
        logger.info(f"âœ… BeautifulSoup ile Ã§ekildi: {len(full_text)} karakter")
        return full_text, image_url
        
    except Exception as e:
        logger.error(f"âŒ BeautifulSoup scrape hatasÄ±: {e}")
        return None, None


def scrape_article_content(url: str):
    """
    ğŸ¯ ADVANCED SCRAPING: newspaper3k + BeautifulSoup kombinasyonu
    """
    try:
        user_agent = random.choice(USER_AGENTS)
        
        # 1ï¸âƒ£ Ã–nce newspaper3k dene
        config = NewspaperConfig()
        config.browser_user_agent = user_agent
        config.request_timeout = 20
        config.fetch_images = True
        config.memoize_articles = False
        
        article = Article(url, language='tr', config=config)
        article.download()
        article.parse()
        
        full_text = article.text.strip()
        scraped_image = article.top_image if article.top_image else None
        
        # 2ï¸âƒ£ Ä°Ã§erik Ã§ok kÄ±saysa BeautifulSoup ile tekrar dene
        if len(full_text) < 800:  # 800 karakterden az = EKSÄ°K Ä°Ã‡ERÄ°K
            logger.warning(f"âš ï¸ Newspaper kÄ±sa Ã§ekti ({len(full_text)} char), BeautifulSoup deneniyor...")
            
            bs_text, bs_image = scrape_with_beautifulsoup(url)
            
            # Hangisi daha uzunsa onu kullan
            if bs_text and len(bs_text) > len(full_text):
                logger.info(f"âœ… BeautifulSoup daha iyi sonuÃ§ verdi: {len(bs_text)} > {len(full_text)}")
                full_text = bs_text
                if bs_image and not scraped_image:
                    scraped_image = bs_image
        
        # 3ï¸âƒ£ Hala Ã§ok kÄ±saysa baÅŸarÄ±sÄ±z say
        if len(full_text) < 300:
            logger.warning(f"âš ï¸ Ä°Ã§erik hala Ã§ok kÄ±sa ({len(full_text)} char), baÅŸarÄ±sÄ±z sayÄ±lÄ±yor")
            return None, None
        
        logger.info(f"âœ… BAÅARILI SCRAPE: {len(full_text)} karakter Ã§ekildi")
        return full_text, scraped_image
        
    except Exception as e:
        logger.error(f"âŒ Scrape hatasÄ± ({url}): {e}")
        return None, None


def scrape_latest_news(count=15):
    """
    En son scrape edilmemiÅŸ haberleri Ã§eker
    """
    logger.info(f"ğŸ” Scrape iÅŸlemi baÅŸlÄ±yor (Hedef: {count})...")
    
    pending_articles = NewsModel.get_unscraped(limit=count, exclude_blacklist=True)
    
    if not pending_articles:
        logger.info("âœ… Scrape edilecek haber yok.")
        return
    
    success = 0
    failed = 0
    
    for idx, article in enumerate(pending_articles, 1):
        try:
            article_url = article['url']
            article_id = article['id']
            
            logger.info(f"ğŸ”„ [{idx}/{len(pending_articles)}] Scraping: {article['title'][:60]}...")
            
            # Ä°Ã§eriÄŸi scrape et
            full_content, scraped_image = scrape_article_content(article_url)
            
            if full_content:
                # Temizle
                try:
                    cleaned_data = full_clean_news_pipeline(
                        title=article.get('title', ''),
                        content=full_content,
                        description=article.get('description'),
                        date=article.get('published')
                    )
                    final_content = cleaned_data['content']
                    
                    # Temizleme sonrasÄ± kontrol
                    if not final_content or len(final_content) < 200:
                        logger.warning(f"âš ï¸ Temizleme sonrasÄ± iÃ§erik Ã§ok kÄ±sa: {article_id}")
                        failed += 1
                        NewsModel.add_to_blacklist(article_url, reason="content_too_short_after_cleaning")
                        continue
                        
                except Exception as clean_err:
                    logger.warning(f"âš ï¸ Temizleme hatasÄ±, ham iÃ§erik kullanÄ±lÄ±yor: {clean_err}")
                    final_content = full_content

                final_image = scraped_image if scraped_image else article.get('image')
                
                # Kaydet
                NewsModel.update_full_content(article_id, final_content, final_image)
                logger.info(f"   âœ… Kaydedildi: {len(final_content)} karakter")
                success += 1
            else:
                failed += 1
                NewsModel.add_to_blacklist(article_url, reason="empty_content")
                logger.info(f"   âŒ BaÅŸarÄ±sÄ±z: Ä°Ã§erik Ã§ekilemedi")
            
            # Rate limiting
            time.sleep(random.uniform(1.5, 3.0))
            
        except Exception as e:
            failed += 1
            logger.error(f"   âŒ DÃ¶ngÃ¼ HatasÄ±: {e}")
    
    logger.info(f"ğŸ‰ Bitti! âœ… BaÅŸarÄ±lÄ±: {success}, âŒ BaÅŸarÄ±sÄ±z: {failed}")


def scrape_in_background(count=15):
    """
    Scraping iÅŸlemini arka planda baÅŸlatÄ±r
    """
    thread = threading.Thread(
        target=scrape_latest_news,
        args=(count,),
        daemon=True
    )
    thread.start()
    logger.info(f"ğŸ”¥ Scraping arka planda baÅŸlatÄ±ldÄ± ({count} haber)")


# ğŸ†• MANUEL TEST FONKSÄ°YONU
def test_single_url(url: str):
    """
    Tek bir URL'i test et (debugging iÃ§in)
    
    KullanÄ±m:
        from services.news_scraper import test_single_url
        test_single_url("https://example.com/haber-linki")
    """
    print(f"\nğŸ” Test ediliyor: {url}\n")
    
    content, image = scrape_article_content(url)
    
    if content:
        print(f"âœ… BAÅARILI!")
        print(f"ğŸ“ Karakter sayÄ±sÄ±: {len(content)}")
        print(f"ğŸ–¼ï¸ Resim: {image}")
        print(f"\nğŸ“° Ä°lk 500 karakter:\n{content[:500]}\n")
        print(f"ğŸ“° Son 500 karakter:\n{content[-500:]}\n")
    else:
        print(f"âŒ BAÅARISIZ - Ä°Ã§erik Ã§ekilemedi")
    
    return content, image
